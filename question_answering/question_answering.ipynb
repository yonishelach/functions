{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75860292-80d3-4dfb-89e4-66579321c78b",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593a39d-6e91-4f92-9e7e-09dcd7dbcab7",
   "metadata": {},
   "source": [
    "## Short description and explenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc0595-8b8a-4a13-b6a7-2a1bc43d8d50",
   "metadata": {},
   "source": [
    "This function enables ad-hoc question answering over documents by ingesting text into a language model and returning formatted responses.<br>\n",
    "It accepts:<br>\n",
    "\n",
    "* A language model<br>\n",
    "* Text files with content<br>\n",
    "* Questions to answer<br>\n",
    "* More inputs can be given for configuration <br>\n",
    "\n",
    "The model processes the files to build understanding. Questions posed are then answered in one of two modes:\n",
    "\n",
    "Default mode: <br>\n",
    "The model directly answers each question using its own capabilities.\n",
    "\n",
    "Poll mode: <br>\n",
    "Additional models are included to separately answer each question. An aggregation algorithm determines the best response through consensus between models.<br>\n",
    "Two options exist for consensus methodology:<br>\n",
    "\n",
    "Average Answer: <br>\n",
    "Each model's answer is scored. The response with the average highest score amongst models is selected. Useful for numeric or ranked responses.\n",
    "\n",
    "Most Common Answer:<br> The answer that occurs most frequently across models is selected. Useful for textual responses to avoid outliers.\n",
    "\n",
    "Using multiple models via the poll mode provides accuracy improvements for questions lacking definitive answers, as it refines responses through an ensemble process. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae957ac3-2c26-4a0b-8e44-8315caeb2953",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a351565-6f2c-4fa3-a024-4b5d658db311",
   "metadata": {},
   "source": [
    "At the core, advanced natural language processing (NLP) models called foundation models are being leveraged to read and comprehend the input text files. <br>\n",
    "Specifically, models such as GPT-3 or Codex from Anthropic are used as the base language model.\n",
    "\n",
    "These foundation models have been trained on huge datasets, allowing them to understand written content, reason about it, and generate text responsively.<br>\n",
    "Underneath, they use techniques like transformer-based deep learning to build semantic representations of the text.\n",
    "\n",
    "When documents are fed into the function, the background process invokes these models to ingest and digest the information.<br>\n",
    "Sophisticated NLP encodings map the textual data into an embedded space that captures latent meanings and relationships.\n",
    "\n",
    "This provides the knowledge base for the models to then offer informed answers tailored to any queries about the documents.<br>\n",
    "The parameters controlling model size and computation time provide tradeoffs between cost, speed, and sophistication of comprehension.\n",
    "\n",
    "Additionally, the poll option expands on a single model by sampling responses from a number of models. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc4aaa-530a-4e9e-8447-737a0cfd6ed5",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d9000-37e1-462b-93c8-1bbfcdf6aaa1",
   "metadata": {},
   "source": [
    "`transformers` <br>\n",
    "`torch` <br>\n",
    "`tqdm` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9b369-1c36-42e8-b106-491ad911f281",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "### Explanation about the function's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3a54d-0cd9-4845-ae14-f24068052bf3",
   "metadata": {},
   "source": [
    "`data_path`:  A path to a directory of text files or a path to a text file to ask questions about. <br>\n",
    "\n",
    "`model_name`: The pre-trained model name from the huggingface hub to use for answering questions. <br>\n",
    "\n",
    "`questions`: The questions to ask. A list of lists of questions to ask per text file, and devided <br>\n",
    "             by question groups, the groups can be determained by size (in order to <br>\n",
    "             avoid large inputs to the llm) or by questioning method (regular or poll like questioning). <br>\n",
    "             \n",
    "`device_map`: A map to use for loading the model on multiple devices. <br>\n",
    "\n",
    "`model_kwargs`: Keyword arguments to pass for loading the model using HuggingFace's <br>\n",
    "                _transformers.AutoModelForCausalLM.from_pretrained_ function. <br>\n",
    "                                           \n",
    "`auto_gptq_exllama_max_input_length`: For AutoGPTQ models to set and extend the model's input buffer size. <br>\n",
    "\n",
    "`tokenizer_name`: The tokenizer name from the huggingface hub to use. If not given, the given model name will be used. <br>\n",
    "                                           \n",
    "`tokenizer_kwargs`: Keyword arguments to pass for loading the tokenizer using HuggingFace's <br>\n",
    "                    _transformers.AutoTokenizer.from_pretrained_ function. <br>\n",
    "                                           \n",
    "`text_wrapper`: Must have a placeholder ('{}') for the text of the file. <br>\n",
    "\n",
    "`questions_wrapper`: A wrapper for the questions received. Will be added after the text wrapper in the prompt template. <br>\n",
    "                     Must have a placeholder ('{}') for the questions. <br>\n",
    "                                           \n",
    "`generation_config`: HuggingFace's _GenerationConfig_ keyword arguments to pass to the _generate_ method. <br>\n",
    "                                           \n",
    "`questions_config`: A dictionary or list of dictionaries containing specific ways to answer questions (using a poll for example), <br>\n",
    "                    each dictionary in the list is for corresponding question group and determines the question asking method <br>\n",
    "                    for said group. <br>\n",
    "                                           \n",
    "`batch_size`: Batch size for inference. <br>\n",
    "\n",
    "`questions_columns`: Columns to use for the dataframe returned. <br>\n",
    "\n",
    "`verbose`: Whether to present logs of a progress bar and errors. Default: True. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e5fac-3def-4cdd-8ca5-d1c93ee64f2e",
   "metadata": {},
   "source": [
    "## Demo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4bc9b-fc5e-4155-8563-0575c22cef05",
   "metadata": {},
   "source": [
    "This is a short and simple example to show the basic use of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95dcfdb-22e1-4b82-b0a3-9c89487a216f",
   "metadata": {},
   "source": [
    "First we import everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60161e5f-468c-47c9-be98-e6554b899c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import transformers\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9744a13-6530-4aa0-a30c-a88db94ce853",
   "metadata": {},
   "source": [
    "We create a text file that the model can be asked about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b874a-0c64-4a66-9b30-fe99191b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_data_dir_for_test():\n",
    "    data_dir = tempfile.mkdtemp()\n",
    "    content = \"The apple is red.\"\n",
    "    with open(data_dir + \"/test_data.txt\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    return data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadd06e-210b-45aa-b7ea-686058b6e7f4",
   "metadata": {},
   "source": [
    "Then we set where to take the path to the text file we want to ask about, the questions, and column name for the answer table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a634b19-d809-4436-bbdd-469fc1d61c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = _make_data_dir_for_test()\n",
    "question = [\"What is the color of the apple?\"]\n",
    "column_name = [\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34be78-c093-4b3b-a542-3f1ca6348fd3",
   "metadata": {},
   "source": [
    "Now we will create the mlrun project, in this project we will set the function to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267b60b-35d1-48bf-8ea0-dfe7a5f366e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=\"call-center-demo-1\",\n",
    "    context=\"./\",\n",
    "    user_project=True,\n",
    "    parameters={\n",
    "        \"default_image\": \"yonishelach/mlrun-llm\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8b39b-433c-40b8-9260-94923c9cbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = project.set_function(\n",
    "    \"question-answering.py\",\n",
    "    name=\"question-answering\",\n",
    "    kind=\"job\",\n",
    "    image=\"yonishelach/mlrun-llm\",\n",
    "    handler=\"answer_questions\",\n",
    ")\n",
    "# project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364ce68-079e-4769-89b6-661fcdc1d475",
   "metadata": {},
   "source": [
    "Now we run the function with all the parameters we prepered earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448bada9-8b52-4175-9839-ecb409ab3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo1_run = func.run(\n",
    "    handler=\"answer_questions\",\n",
    "    params={\n",
    "        \"model\": \"distilgpt2\",\n",
    "        \"input_path\": input_path,\n",
    "        \"questions\": question,\n",
    "        \"questions_columns\": column_name,\n",
    "        \"generation_config\": {\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_p\": 0.9,\n",
    "            \"early_stopping\": True,\n",
    "            \"max_new_tokens\": 20,\n",
    "        },\n",
    "    },\n",
    "    returns=[\n",
    "        \"question_answering_df: dataset\",\n",
    "        \"question_answering_errors: result\",\n",
    "    ],\n",
    "    local=True,\n",
    "    artifact_path=\"./\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474505db-2fc8-48fd-a634-2bada802a449",
   "metadata": {},
   "source": [
    "and after the run is finished we can take a look and see our answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560b51d-5f96-465d-9826-e88c7d4d46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo1_run.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a401a5-2f8a-427f-bf62-2f31f94f5ee7",
   "metadata": {},
   "source": [
    "## Demo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b8a40-ad61-445f-900b-4fdaa036e417",
   "metadata": {},
   "source": [
    "This is a much larger example, we will show how we use this function to analyze a number of calls between agents and customer of a internet company (all the data is generated by Iguazio). <br>\n",
    "For something like this, we recomend using a strong model, and putting some time into making the prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c521b-df3d-498f-8642-863182107618",
   "metadata": {},
   "source": [
    "We start with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde6a480-a3d9-4b8c-a9c0-daa235f0f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-12-18 10:18:37,490 [warning] Client version with higher version than server version isn't supported, align your client to the server version: {'parsed_server_version': Version(major=1, minor=5, patch=2, prerelease='rc1', build='track'), 'parsed_client_version': Version(major=1, minor=6, patch=0, prerelease='rc11', build=None)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlrun\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3ebb2-7d4a-4e52-89ed-45287c06eb76",
   "metadata": {},
   "source": [
    "This example is a bit more complicated as we mentioned, we give the model a list of questions, for some of them we give the model a list of answers to choose from (e.g. TOPICS, TONES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc065e4-2dbf-4d7a-9772-6b7039f428bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\n",
    "    \"slow internet speed\",\n",
    "    \"billing discrepancies\",\n",
    "    \"account login problems\",\n",
    "    \"setting up a new device\",\n",
    "    \"phishing or malware concerns\",\n",
    "    \"scheduled maintenance notifications\",\n",
    "    \"service upgrades\",\n",
    "    \"negotiating pricing\",\n",
    "    \"canceling service\",\n",
    "    \"customer service feedback\",\n",
    "]\n",
    "\n",
    "TONES = [\n",
    "    \"Positive\",\n",
    "    \"Neutral\",\n",
    "    \"Negative\",\n",
    "]\n",
    "\n",
    "QUESTIONS = [\n",
    "    [\n",
    "    f\"1. Classify the topic of the text from the following list (choose one): {TOPICS}\",\n",
    "    \"2. Write a long summary of the text, focus on the topic (max 50 words).\",\n",
    "    \"3. Was the Client's concern addressed, (choose only one) [Yes, No]?\",\n",
    "    f\"4. Was the Client tone (choose only one, if not sure choose Neutral) {TONES}? \",\n",
    "    f\"5. Was the Call Center Agent tone (choose only one, if not sure choose Neutral) {TONES}?\",\n",
    "    ],\n",
    "    [\n",
    "    \"1. Did the agent try to upsale the customer (choose only one) [Yes, No]? (sell any additional product or service)\",\n",
    "    \"2. If the agent indeed try to upsale the client, did he succeed (choose only one) [Yes, No]? if he didn't try' answer No\",\n",
    "    \"3. Rate the agent's level of empathy (The ability to understand and share the feelings of others) on a scale of 1-5.\",\n",
    "    \"4. Rate the agent's level of professionalism (Conducting oneself in a way that is appropriate for the workplace) on a scale of 1-5.\",\n",
    "    \"5. Rate the agent's level of kindness (The quality of being friendly, generous, and considerate) on a scale of 1-5.\",\n",
    "    \"6. Rate the agent's level of effective communication (The ability to convey information clearly and concisely) on a scale of 1-5.\",\n",
    "    \"7. Rate the agent's level of active listening (The process of paying attention to and understanding what someone is saying) on a scale of 1-5.\",\n",
    "    \"8. Rate the agent's level of customization (The process of tailoring something to the specific needs or preferences of an individual) on a scale of 1-5.\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "qa_questions_columns = [\n",
    "                        \"Issue\",\n",
    "                        \"Summary\",\n",
    "                        \"is_fixed\",\n",
    "                        \"customer_tone\",\n",
    "                        \"agent_tone\",\n",
    "                        \"upsale_attemted\",\n",
    "                        \"upsale_success\",\n",
    "                        \"empathy\",\n",
    "                        \"professionalism\",\n",
    "                        \"kindness\",\n",
    "                        \"effective_communication\",\n",
    "                        \"active_listening\",\n",
    "                        \"customization\",\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89f316-0d1b-4ada-9990-d2293546eee3",
   "metadata": {},
   "source": [
    "Another thing we give the model this time is answer examples (one/few shot answering), this can be done to show the model how you want the answer to be structured or caculated. <br>\n",
    "So for every file we ask about, the model will be presented with this example of a call and how we want the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc093ad-dab4-46a1-b36a-2a7551cef018",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_CALL = (\n",
    "    \"Agent: Good afternoon, you've reached [Internet Service Provider] customer support. I'm Megan. How can I assist \"\n",
    "    \"you today?\\n\"\n",
    "    \"Customer: Hello, Megan. This is Lisa. I've noticed some billing discrepancies on my last statement.\\n\"\n",
    "    \"Agent: I'm sorry to hear that, Lisa. I'd be happy to help you with that. Could you please provide me with your \"\n",
    "    \"account number or phone number associated with your account?\\n\"\n",
    "    \"Customer: Of course, my account number is 123456789.\\n\"\n",
    "    \"Agent: Thank you, Lisa. Let me pull up your account. I see the billing discrepancies you mentioned. It appears \"\n",
    "    \"there was an error in the charges. I apologize for the inconvenience.\\n\"\n",
    "    \"Customer: Thank you for acknowledging the issue, Megan. Can you please help me get it resolved?\\n\"\n",
    "    \"Agent: Absolutely, Lisa. I've made note of the discrepancies, and I'll escalate this to our billing department \"\n",
    "    \"for investigation and correction. You should see the adjustments on your next statement.\\n\"\n",
    "    \"Customer: That sounds good, Megan. I appreciate your help.\\n\"\n",
    "    \"Agent: You're welcome, Lisa. If you have any more questions or concerns in the future, please don't hesitate to \"\n",
    "    \"reach out. Is there anything else I can assist you with today?\\n\"\n",
    "    \"Customer: No, that's all. Thank you for your assistance, Megan.\\n\"\n",
    "    \"Agent: Not a problem, Lisa. Have a wonderful day, and we'll get this sorted out for you.\\n\"\n",
    "    \"Customer: You too! Goodbye, Megan.\\n\"\n",
    "    \"Agent: Goodbye, Lisa!\"\n",
    ")\n",
    "\n",
    "DEMO_ANSWERS = [(\n",
    "    \"1. billing discrepancies\\n\"\n",
    "    \"2. The customer, contacted the call center regarding billing discrepancies on her statement. The agent, \"\n",
    "    \"acknowledged the issue, assured The customer it would be resolved, and escalated it to the billing department for \"\n",
    "    \"correction.\\n\"\n",
    "    \"3. Yes.\\n\"\n",
    "    \"4. Natural.\\n\"\n",
    "    \"5. positive.\\n\"),\n",
    "    # \"6. No\\n\"\n",
    "    # \"7. No\\n\"),\n",
    "    (\n",
    "    \"1. No\\n\"\n",
    "    \"2. No\\n\"\n",
    "    \"3. 4\\n\"\n",
    "    \"4. 5\\n\"\n",
    "    \"5. 4\\n\"\n",
    "    \"6. 5\\n\"\n",
    "    \"7. 4\\n\"\n",
    "    \"8. 3\"\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44ded3-fee3-4911-a02a-6a51a62a7020",
   "metadata": {},
   "source": [
    "Then we need to wrap it all nicely to be given to the model as a single prompt, this is done with a text wrapper, and a question wrapper. <br>\n",
    "both of them will be concatenated inside the function with the questions and passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108f5aa-75a6-402d-83a6-bf45f0d7223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_WRAPPER = [(\n",
    "    f\"<|im_start|>system: You are an AI assistant that answers questions accurately and shortly<|im_end|>\\n\"\n",
    "    f\"<|im_start|>user: Given the following text:\\n\"\n",
    "    f\"{DEMO_CALL}\\n\"\n",
    "    f\"answer the questions as accurately as you can:\\n\"\n",
    "    f\"{QUESTIONS[i]}<|im_end|>\\n\"\n",
    "    f\"<|im_start|>assistant:\\n\"\n",
    "    f\"{DEMO_ANSWERS[i]}<|im_end|>\\n\"\n",
    "    f\"<|im_start|>user: Given the following text:\\n\"\n",
    "    \"{}\"\n",
    ") for i in range(len(QUESTIONS))]\n",
    "QUESTIONS_WRAPPER = (\n",
    "    \" answer the given questions as accurately as you can, do not write more answers the questions:\\n\"\n",
    "    \"{}<|im_end|>\\n\"\n",
    "    \"<|im_start|>assistant:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91fae9-4179-440b-acfd-aec2c3c9f031",
   "metadata": {},
   "source": [
    "Another interesting thing we do in this example is use two different questioning methods, the first is the \"default\" way (represented by a black dictionary). <br>\n",
    "The second part of the config is for the second questioning method, we cal \"poll\", and in which we need to choose how many voting models we want participating,<br>\n",
    "and in what way we want do decide the result, we currentlly support `average` and `most_common` as show here.<br>\n",
    "\n",
    "\n",
    "*An explenation about both methods can be found in the begginig of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cac92b3-7fb4-4c88-a176-0dfd9e540dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_config = [\n",
    "    {},\n",
    "    {\n",
    "        \"type\": \"poll\",\n",
    "        \"poll_count\": 3,\n",
    "        \"poll_strategy\": \"most_common\"\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913f6d9-3be5-4aba-8cac-b724e3b3d9ba",
   "metadata": {},
   "source": [
    "Before we run the function using mlrun, we need to set a project, and our function in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187a4643-53e9-40bb-a337-5096df7946d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-12-18 10:18:51,651 [info] Project loaded successfully: {'project_name': 'call-center-demo-zeev55'}\n"
     ]
    }
   ],
   "source": [
    "project = mlrun.get_or_create_project(\n",
    "    name=\"call-center-demo-2\",\n",
    "    context=\"./\",\n",
    "    user_project=True,\n",
    "    parameters={\n",
    "        \"default_image\": \"yonishelach/mlrun-llm\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17eb7783-9ced-482b-9bdf-c41e55995faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.projects.project.MlrunProject at 0x7f8bc5b0a370>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = project.set_function(\n",
    "    \"question-answering.py\",\n",
    "    name=\"question-answering\",\n",
    "    kind=\"job\",\n",
    "    image=\"yonishelach/mlrun-llm\",\n",
    "    handler=\"answer_questions\",\n",
    ")\n",
    "project.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44b391-87d2-447d-aafa-66ed45f06ba5",
   "metadata": {},
   "source": [
    "The last few parameters we need to set are the model we will use, the input lenth (no available for all models) and the batch size. <br>\n",
    "The batch size determains how many files we want procced at each epoch, and the larger we go the faster the proccess will be, as long as our memor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cae4c-541b-49a3-b24d-deb94f7130fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
    "auto_gptq_exllama_max_input_length = 4096\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa4eaa-f3b0-457f-b98a-18a8ee5ba4d8",
   "metadata": {},
   "source": [
    "Finnaly, we run the function with all the parameters we prepared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d200706-e852-4ce9-9b9a-61686b30e5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-12-18 14:33:45,366 [info] Storing function: {'name': 'question-answering-answer-questions', 'uid': '0195a27963d94fb9805084e44b331162', 'db': 'http://mlrun-api:8080'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>call-center-demo-zeev55-zeevr</td>\n",
       "      <td><div title=\"0195a27963d94fb9805084e44b331162\"><a href=\"https://dashboard.default-tenant.app.llm2.iguazio-cd0.com/mlprojects/call-center-demo-zeev55-zeevr/jobs/monitor/0195a27963d94fb9805084e44b331162/overview\" target=\"_blank\" >...4b331162</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 18 14:33:45</td>\n",
       "      <td>completed</td>\n",
       "      <td>question-answering-answer-questions</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=zeevr</div><div class=\"dictlist\">kind=local</div><div class=\"dictlist\">owner=zeevr</div><div class=\"dictlist\">host=jupyter-zeev-gpu-5995df47dc-2ps5g</div></td>\n",
       "      <td><div title=\"/User/call-center/demo-call-center/calls\">data_path</div></td>\n",
       "      <td><div class=\"dictlist\">model_name=TheBloke/Mistral-7B-OpenOrca-GPTQ</div><div class=\"dictlist\">model_kwargs={}</div><div class=\"dictlist\">device_map=auto</div><div class=\"dictlist\">text_wrapper=['<|im_start|>system: You are an AI assistant that answers questions accurately and shortly<|im_end|>\\n<|im_start|>user: Given the following text:\\nAgent: Good afternoon, you\\'ve reached [Internet Service Provider] customer support. I\\'m Megan. How can I assist you today?\\nCustomer: Hello, Megan. This is Lisa. I\\'ve noticed some billing discrepancies on my last statement.\\nAgent: I\\'m sorry to hear that, Lisa. I\\'d be happy to help you with that. Could you please provide me with your account number or phone number associated with your account?\\nCustomer: Of course, my account number is 123456789.\\nAgent: Thank you, Lisa. Let me pull up your account. I see the billing discrepancies you mentioned. It appears there was an error in the charges. I apologize for the inconvenience.\\nCustomer: Thank you for acknowledging the issue, Megan. Can you please help me get it resolved?\\nAgent: Absolutely, Lisa. I\\'ve made note of the discrepancies, and I\\'ll escalate this to our billing department for investigation and correction. You should see the adjustments on your next statement.\\nCustomer: That sounds good, Megan. I appreciate your help.\\nAgent: You\\'re welcome, Lisa. If you have any more questions or concerns in the future, please don\\'t hesitate to reach out. Is there anything else I can assist you with today?\\nCustomer: No, that\\'s all. Thank you for your assistance, Megan.\\nAgent: Not a problem, Lisa. Have a wonderful day, and we\\'ll get this sorted out for you.\\nCustomer: You too! Goodbye, Megan.\\nAgent: Goodbye, Lisa!\\nanswer the questions as accurately as you can:\\n[\"1. Classify the topic of the text from the following list (choose one): [\\'slow internet speed\\', \\'billing discrepancies\\', \\'account login problems\\', \\'setting up a new device\\', \\'phishing or malware concerns\\', \\'scheduled maintenance notifications\\', \\'service upgrades\\', \\'negotiating pricing\\', \\'canceling service\\', \\'customer service feedback\\']\", \\'2. Write a long summary of the text, focus on the topic (max 50 words).\\', \"3. Was the Client\\'s concern addressed, (choose only one) [Yes, No]?\", \"4. Was the Client tone (choose only one, if not sure choose Neutral) [\\'Positive\\', \\'Neutral\\', \\'Negative\\']? \", \"5. Was the Call Center Agent tone (choose only one, if not sure choose Neutral) [\\'Positive\\', \\'Neutral\\', \\'Negative\\']?\"]<|im_end|>\\n<|im_start|>assistant:\\n1. billing discrepancies\\n2. The customer, contacted the call center regarding billing discrepancies on her statement. The agent, acknowledged the issue, assured The customer it would be resolved, and escalated it to the billing department for correction.\\n3. Yes.\\n4. Natural.\\n5. positive.\\n<|im_end|>\\n<|im_start|>user: Given the following text:\\n{}', '<|im_start|>system: You are an AI assistant that answers questions accurately and shortly<|im_end|>\\n<|im_start|>user: Given the following text:\\nAgent: Good afternoon, you\\'ve reached [Internet Service Provider] customer support. I\\'m Megan. How can I assist you today?\\nCustomer: Hello, Megan. This is Lisa. I\\'ve noticed some billing discrepancies on my last statement.\\nAgent: I\\'m sorry to hear that, Lisa. I\\'d be happy to help you with that. Could you please provide me with your account number or phone number associated with your account?\\nCustomer: Of course, my account number is 123456789.\\nAgent: Thank you, Lisa. Let me pull up your account. I see the billing discrepancies you mentioned. It appears there was an error in the charges. I apologize for the inconvenience.\\nCustomer: Thank you for acknowledging the issue, Megan. Can you please help me get it resolved?\\nAgent: Absolutely, Lisa. I\\'ve made note of the discrepancies, and I\\'ll escalate this to our billing department for investigation and correction. You should see the adjustments on your next statement.\\nCustomer: That sounds good, Megan. I appreciate your help.\\nAgent: You\\'re welcome, Lisa. If you have any more questions or concerns in the future, please don\\'t hesitate to reach out. Is there anything else I can assist you with today?\\nCustomer: No, that\\'s all. Thank you for your assistance, Megan.\\nAgent: Not a problem, Lisa. Have a wonderful day, and we\\'ll get this sorted out for you.\\nCustomer: You too! Goodbye, Megan.\\nAgent: Goodbye, Lisa!\\nanswer the questions as accurately as you can:\\n[\\'1. Did the agent try to upsale the customer (choose only one) [Yes, No]? (sell any additional product or service)\\', \"2. If the agent indeed try to upsale the client, did he succeed (choose only one) [Yes, No]? if he didn\\'t try\\' answer No\", \"3. Rate the agent\\'s level of empathy (The ability to understand and share the feelings of others) on a scale of 1-5.\", \"4. Rate the agent\\'s level of professionalism (Conducting oneself in a way that is appropriate for the workplace) on a scale of 1-5.\", \"5. Rate the agent\\'s level of kindness (The quality of being friendly, generous, and considerate) on a scale of 1-5.\", \"6. Rate the agent\\'s level of effective communication (The ability to convey information clearly and concisely) on a scale of 1-5.\", \"7. Rate the agent\\'s level of active listening (The process of paying attention to and understanding what someone is saying) on a scale of 1-5.\", \"8. Rate the agent\\'s level of customization (The process of tailoring something to the specific needs or preferences of an individual) on a scale of 1-5.\"]<|im_end|>\\n<|im_start|>assistant:\\n1. No\\n2. No\\n3. 4\\n4. 5\\n5. 4\\n6. 5\\n7. 4\\n8. 3<|im_end|>\\n<|im_start|>user: Given the following text:\\n{}']</div><div class=\"dictlist\">questions=[[\"1. Classify the topic of the text from the following list (choose one): ['slow internet speed', 'billing discrepancies', 'account login problems', 'setting up a new device', 'phishing or malware concerns', 'scheduled maintenance notifications', 'service upgrades', 'negotiating pricing', 'canceling service', 'customer service feedback']\", '2. Write a long summary of the text, focus on the topic (max 50 words).', \"3. Was the Client's concern addressed, (choose only one) [Yes, No]?\", \"4. Was the Client tone (choose only one, if not sure choose Neutral) ['Positive', 'Neutral', 'Negative']? \", \"5. Was the Call Center Agent tone (choose only one, if not sure choose Neutral) ['Positive', 'Neutral', 'Negative']?\"], ['1. Did the agent try to upsale the customer (choose only one) [Yes, No]? (sell any additional product or service)', \"2. If the agent indeed try to upsale the client, did he succeed (choose only one) [Yes, No]? if he didn't try' answer No\", \"3. Rate the agent's level of empathy (The ability to understand and share the feelings of others) on a scale of 1-5.\", \"4. Rate the agent's level of professionalism (Conducting oneself in a way that is appropriate for the workplace) on a scale of 1-5.\", \"5. Rate the agent's level of kindness (The quality of being friendly, generous, and considerate) on a scale of 1-5.\", \"6. Rate the agent's level of effective communication (The ability to convey information clearly and concisely) on a scale of 1-5.\", \"7. Rate the agent's level of active listening (The process of paying attention to and understanding what someone is saying) on a scale of 1-5.\", \"8. Rate the agent's level of customization (The process of tailoring something to the specific needs or preferences of an individual) on a scale of 1-5.\"]]</div><div class=\"dictlist\">questions_wrapper= answer the given questions as accurately as you can, do not write more answers the questions:\\n{}<|im_end|>\\n<|im_start|>assistant:\\n</div><div class=\"dictlist\">questions_columns=['Issue', 'Summary', 'is_fixed', 'customer_tone', 'agent_tone', 'upsale_attemted', 'upsale_success', 'empathy', 'professionalism', 'kindness', 'effective_communication', 'active_listening', 'customization']</div><div class=\"dictlist\">generation_config={'max_new_tokens': 250, 'do_sample': True, 'temperature': 0.7, 'top_p': 0.95, 'top_k': 40, 'repetition_penalty': 1.1}</div><div class=\"dictlist\">questions_config=[{}, {'poll_count': 3, 'poll_strategy': 'most_common'}]</div><div class=\"dictlist\">batch_size=1</div><div class=\"dictlist\">auto_gptq_exllama_max_input_length=4096</div></td>\n",
       "      <td><div class=\"dictlist\">question_answering_errors={'.ipynb_checkpoints': \"[Errno 21] Is a directory: '/User/call-center/demo-call-center/calls/.ipynb_checkpoints'\"}</div></td>\n",
       "      <td><div title=\"v3io:///projects/call-center-demo-zeev55-zeevr/artifacts/question-answering-answer-questions/0/question_answering_df.parquet\">question_answering_df</div></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result0a8632b5-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result0a8632b5-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result0a8632b5\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result0a8632b5-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.llm2.iguazio-cd0.com/mlprojects/call-center-demo-zeev55-zeevr/jobs/monitor/0195a27963d94fb9805084e44b331162/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2023-12-18 14:43:08,462 [info] Run execution finished: {'status': 'completed', 'name': 'question-answering-answer-questions'}\n"
     ]
    }
   ],
   "source": [
    "# Question answering:\n",
    "run = func.run(\n",
    "    function=\"question-answering\",\n",
    "    local=True,\n",
    "    handler=\"answer_questions\",\n",
    "    inputs={\"data_path\": os.path.abspath(\"./calls\")},\n",
    "    params={\n",
    "        \"model_name\": qa_model,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"text_wrapper\":TEXT_WRAPPER,\n",
    "        \"questions\": QUESTIONS,\n",
    "        \"questions_wrapper\": QUESTIONS_WRAPPER,\n",
    "        \"questions_columns\": qa_questions_columns,\n",
    "        \"generation_config\": {\n",
    "            \"max_new_tokens\": 250,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\":0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 40,\n",
    "            \"repetition_penalty\": 1.1\n",
    "        },\n",
    "        \"questions_config\": questions_config,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"auto_gptq_exllama_max_input_length\": auto_gptq_exllama_max_input_length,\n",
    "    },\n",
    "    returns=[\n",
    "        \"question_answering_df: dataset\",\n",
    "        \"question_answering_errors: result\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fa39c5bf-c959-4ff5-ad60-4ad68d00f22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_answering_errors': {'10ca54de28508a7c62453f618e55fdd3.txt, 1765b93c42a9343ed9a1da8f93d411e3.txt': 'list.extend() takes exactly one argument (2 given)',\n",
       "  '2df61e0a2814c75b50e2c040996f5a61.txt, 339fcd0663e93bc6530dbf4987e1d4bc.txt': 'list.extend() takes exactly one argument (2 given)'},\n",
       " 'question_answering_df': 'store://artifacts/call-center-demo-zeev55-zeevr/question-answering-answer-questions_question_answering_df:f6081a7ff60a4a90bdc84e1f7dfb0d4d'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b037f-0831-4a33-bbd7-83fa887a461d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd120cd-4f92-4fa2-bc8f-1c7b04fb4280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4717db-e557-4f3b-a0e3-72dc478e4350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c44aa-4d26-4837-9d3a-1f45329fdb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
